{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries to install\n",
    "```\n",
    "$ pip install spacy\n",
    "$ python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "## Whar is sentiment analysis?\n",
    "\n",
    "**Sentiment Analysis** is the process of analysing text with the aim of trying to understand the context of the text or the opinion expressed within a block of text. Its a tool that allows computers to understand the underlying tone in a given text. Sentiment analysis give computers the abilily to understand human text. This can be difficult to achieve since its even difficult for humans to understand text coming to talk of computers it gets even harder. But there are tools and techniques to do that.\n",
    "\n",
    "## Why sentiment analysis\n",
    "\n",
    "1. To understand customers opinion on a certain service or product.\n",
    "\n",
    "2. Useful in social media monitoring\n",
    "\n",
    "3. Used to process data to detect opinions and emotions of a group of people\n",
    "\n",
    "## Objectives of this tutorial\n",
    "\n",
    "1. Learn Natual language processing techniques\n",
    "\n",
    "2. Use ML to determine the sentiment of a text\n",
    "\n",
    "3. How to use spaCy in sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing to Clean data\n",
    "\n",
    "NLP just like any other ML algorithm requiers a well cleaned dataset. The main aim of this is to reduce the noise in the text data that is inherent in human text. There are many tools to do this some of the common ones include Natural language Toolkit, TextBlob  and SpaCy. In this tutorial we will be using spaCy. In NLP we use the following process to clean our data:\n",
    "\n",
    "1. Tokenizing sentences\n",
    "\n",
    "2. Removing stop words like “if,” “but,” “or,” and so on\n",
    "\n",
    "3. Normalizing words\n",
    "\n",
    "4. Vectorizing text\n",
    "\n",
    "\n",
    "\n",
    "## Terminologies\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "This is the technique or process of breaking down a sentence or a chunk of text into smaller pieces. Tokenization is the first step in NLP pipeline. There are mainly two forms or types of tokenization, this include:\n",
    "\n",
    "#### Word Tokenization \n",
    "\n",
    "This type of tokenization involves breaking a text into individual words.\n",
    "\n",
    "#### Sentence Tokenization\n",
    "\n",
    "This involves breaking down a text into a individual of sentences\n",
    "\n",
    "#### Example of word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[,\n",
       " Dave,\n",
       " watched,\n",
       " as,\n",
       " the,\n",
       " forest,\n",
       " burned,\n",
       " up,\n",
       " on,\n",
       " the,\n",
       " hill,\n",
       " ,,\n",
       " ,\n",
       " only,\n",
       " a,\n",
       " few,\n",
       " miles,\n",
       " from,\n",
       " his,\n",
       " house,\n",
       " .,\n",
       " The,\n",
       " car,\n",
       " had,\n",
       " ,\n",
       " been,\n",
       " hastily,\n",
       " packed,\n",
       " and,\n",
       " Marta,\n",
       " was,\n",
       " inside,\n",
       " trying,\n",
       " to,\n",
       " round,\n",
       " ,\n",
       " up,\n",
       " the,\n",
       " last,\n",
       " of,\n",
       " the,\n",
       " pets,\n",
       " .,\n",
       " \",\n",
       " Where,\n",
       " could,\n",
       " she,\n",
       " be,\n",
       " ?,\n",
       " \",\n",
       " he,\n",
       " wondered,\n",
       " ,\n",
       " as,\n",
       " he,\n",
       " continued,\n",
       " to,\n",
       " wait,\n",
       " for,\n",
       " Marta,\n",
       " to,\n",
       " appear,\n",
       " with,\n",
       " the,\n",
       " pets,\n",
       " .,\n",
       " ]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "text = \"\"\"\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets.\n",
    "\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "token_list = [token for token in doc]\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Word\n",
    "\n",
    "stop words are characters that might be useful in human communication but are not important in sentiment analysis in a machine learning model or a deep learning model. We need to remove these words example \"the\", \"as\"...\n",
    "\n",
    "SpaCy come with a defual list of this stop words that we can use to remove stop words from the tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[,\n",
       " Dave,\n",
       " watched,\n",
       " forest,\n",
       " burned,\n",
       " hill,\n",
       " ,,\n",
       " ,\n",
       " miles,\n",
       " house,\n",
       " .,\n",
       " car,\n",
       " ,\n",
       " hastily,\n",
       " packed,\n",
       " Marta,\n",
       " inside,\n",
       " trying,\n",
       " round,\n",
       " ,\n",
       " pets,\n",
       " .,\n",
       " \",\n",
       " ?,\n",
       " \",\n",
       " wondered,\n",
       " ,\n",
       " continued,\n",
       " wait,\n",
       " Marta,\n",
       " appear,\n",
       " pets,\n",
       " .,\n",
       " ]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Now that we have removed the unnecessary word, lets normalize the remaining text. Normalization is the process of condensing many variations of a single word into one single variation. Example take the words \"wait\", \"waited\" and \"waiting\" will be condensed into \"wait\".\n",
    "\n",
    "#### Types of Normalization\n",
    "\n",
    "There are two main types of normalization:\n",
    "\n",
    "1. Stemming\n",
    "\n",
    "With this method, a word is trimmed off at its stem, like in the two examples we looked at earlier. Example \"wait\", \"waiting\" and \"waited\" have a common base of \"wait\". The relationship between \"drive\" and \"drove\" will be missed.\n",
    "\n",
    "2. Lemmatization\n",
    "\n",
    "This is a more powerful form of stemming, Lemmatization considers the context and converts the word to its base meaningful form which is called lemma.\n",
    "\n",
    "Lemmatization can also be said as, the process of converting the given word into it's base form according to the dictionary meaning of the word.\n",
    "\n",
    "Example: \"studying\" and \"studies\" all have the dictionary or the meaninful meaning of \"study\"\n",
    "\n",
    "Since lemmatization is more powerful and efficient than stemming spaCy only provides lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token: \\n, lemma: \\n',\n",
       " 'token: Dave, lemma: Dave',\n",
       " 'token: watched, lemma: watch',\n",
       " 'token: forest, lemma: forest',\n",
       " 'token: burned, lemma: burn',\n",
       " 'token: hill, lemma: hill',\n",
       " 'token: ,, lemma: ,',\n",
       " 'token: \\n, lemma: \\n',\n",
       " 'token: miles, lemma: mile',\n",
       " 'token: house, lemma: house',\n",
       " 'token: ., lemma: .',\n",
       " 'token: car, lemma: car',\n",
       " 'token: \\n, lemma: \\n',\n",
       " 'token: hastily, lemma: hastily',\n",
       " 'token: packed, lemma: pack',\n",
       " 'token: Marta, lemma: Marta',\n",
       " 'token: inside, lemma: inside',\n",
       " 'token: trying, lemma: try',\n",
       " 'token: round, lemma: round',\n",
       " 'token: \\n, lemma: \\n',\n",
       " 'token: pets, lemma: pet',\n",
       " 'token: ., lemma: .',\n",
       " 'token: \", lemma: \"',\n",
       " 'token: ?, lemma: ?',\n",
       " 'token: \", lemma: \"',\n",
       " 'token: wondered, lemma: wonder',\n",
       " 'token: \\n, lemma: \\n',\n",
       " 'token: continued, lemma: continue',\n",
       " 'token: wait, lemma: wait',\n",
       " 'token: Marta, lemma: Marta',\n",
       " 'token: appear, lemma: appear',\n",
       " 'token: pets, lemma: pet',\n",
       " 'token: ., lemma: .',\n",
       " 'token: \\n, lemma: \\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma = [f\"token: {token}, lemma: {token.lemma_}\" for token in filtered_tokens]\n",
    "lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Vectorization is the process of converting the tokenized words into a numeric array of numbers which is unique to a given token and  represents various features of a token. Vecotrs are used to find the similarities within words, classify text and perform other NLP operations.\n",
    "\n",
    "The arrays in that represent a vector for a given token can either be **densed array** in which very space in the array contains a defined value. On the other hand **sparsed array** most of the spaces in the array is empty. In most cases we'll use dense array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dave"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8371642 ,  1.452925  , -1.6147203 ,  0.67836225, -0.659443  ,\n",
       "        1.6417911 ,  0.57964015,  2.3021278 , -0.13260579,  0.57509375,\n",
       "        1.5654867 , -0.69388777, -0.5960694 , -1.5377433 ,  1.9425607 ,\n",
       "       -2.4552503 ,  1.2321602 ,  1.0434954 , -1.5102386 , -0.57876253,\n",
       "        0.12055516,  3.6501799 ,  2.616098  , -0.57102156, -1.5221778 ,\n",
       "        0.0062914 ,  0.22760749, -1.9220744 , -1.6252842 , -4.2262235 ,\n",
       "       -3.495663  , -3.3120532 ,  0.81387675, -0.00677478, -0.11603296,\n",
       "        1.462044  ,  3.0751472 ,  0.35958475, -0.22526968, -2.7439258 ,\n",
       "        1.2696334 ,  4.606787  ,  0.3403422 , -2.127231  ,  1.261918  ,\n",
       "       -4.209798  ,  5.4528546 ,  1.6940243 , -2.597298  ,  0.95049405,\n",
       "       -1.9105787 , -2.374928  , -1.4227569 , -2.2528832 , -1.7998077 ,\n",
       "        1.607501  ,  2.9914231 ,  2.8065157 , -1.2510273 , -0.5496425 ,\n",
       "       -0.49980426, -1.3882611 , -0.47047865, -2.9670255 ,  1.7884939 ,\n",
       "        4.5282784 , -1.2602415 , -0.14885461,  1.0419188 , -0.08892578,\n",
       "       -1.1382759 ,  2.2426186 ,  1.5077226 , -1.5030198 ,  2.5280986 ,\n",
       "       -1.6761327 ,  0.16694829,  2.12396   ,  0.02546155,  0.3875456 ,\n",
       "        0.89119613, -0.07678461, -2.0690749 , -1.121186  ,  1.4821008 ,\n",
       "        1.1989202 ,  2.1933234 ,  0.52963793,  3.0646472 , -1.722329  ,\n",
       "       -1.3634226 , -0.47471   , -1.7648505 ,  3.5651777 , -2.3942056 ,\n",
       "       -1.3800393 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[1].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[1].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy For Text Classification\n",
    "\n",
    "SpaCy has alot of this inbuilt functions to preprocess data, it also provides alot of pipeline functions to enable us to label our data.The default pipeline is defined in a JSON file associated with whichever preexisting model you’re using (en_core_web_sm for this tutorial), but you can also build one from scratch if you wish.\n",
    "\n",
    "One of the built-in pipeline functions that spaCy provides is called textcat (TextCategorizer), which enables you to assign categories(labels) to your text data and use that as training data for a neural network or a machine learning model.\n",
    "\n",
    "This process will generate a trained model that you can then use to predict the sentiment of a given piece of text.\n",
    "\n",
    "To achieve this we need to follow the following steps:\n",
    "\n",
    "1. Add the textcat component to the existing pipeline.\n",
    "2. Add valid labels to the textcat component.\n",
    "3. Load, shuffle, and split your data.\n",
    "4. Train the model, evaluating on each training loop.\n",
    "5. Use the trained model to predict the sentiment of non-training data.\n",
    "6. Optionally, save the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Sentiment Analysis\n",
    "\n",
    "To build a sentiment analysis pipline you need to follow the following steps or include the following in your pipelne.\n",
    "\n",
    "1. Loading data\n",
    "2. Preprocessing\n",
    "3. Training the classifier\n",
    "4. Classifying data\n",
    "\n",
    "We will build a movie sentiment analyzer, the dataset well be using is [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "## Loading data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def load_training_data(\n",
    "    data_directory: str = \"./datasets/aclImdb_v1/aclImdb/train\",\n",
    "    split: float = 0.8,\n",
    "    limit: int = 0\n",
    ") -> tuple:\n",
    "     # Load from files\n",
    "    reviews = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\") as f:\n",
    "                    text = f.read()\n",
    "                    text = text.replace(\"<br />\", \"\\n\\n\")\n",
    "                    if text.strip():\n",
    "                        spacy_label = {\n",
    "                            \"cats\": {\n",
    "                                \"pos\": \"pos\" == label,\n",
    "                                \"neg\": \"neg\" == label\n",
    "                            }\n",
    "                        }\n",
    "                        reviews.append((text, spacy_label))\n",
    "    random.shuffle(reviews)\n",
    "\n",
    "    if limit:\n",
    "        reviews = reviews[:limit]\n",
    "    split = int(len(reviews) * split)\n",
    "    return reviews[:split], reviews[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = load_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been able to load the datasets and do some simple processing on the data such as assigning labels to the text we loaded. Now we have a dataset containing labled text. We have also shuffled the data at this point to ensure that ordering does not affect the model accuracy in anyway and also to get the dataset well mixed up before we split the data into training and testing sets.\n",
    "\n",
    "To split the data we use list slicing to do that, we obtain the value used to slice the list by using the argument passed into the function and t multiply it with the total lenght of the reviews list. We return the first 80% of that value and the rest 80% and above.\n",
    "\n",
    "#### NOTE:\n",
    "\n",
    "The label dictionary structure is a format required by the spaCy model during the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I rented this because I'm a bit weary of '80s NBC programming and apparently I saved myself a lot of money. I have nothing against any of the actors and for their credit they do a good job but this show is flawed from the premise.\\n\\n\\n\\nWe have a character who is unlikable. He's full of flaws, not enlightened, and a complete jerk on a good day. Yet the reason why anybody should care just isn't there. While creating an American sitcom centered around a complete bullheaded jackass is revolutionary and full of potential, it just isn't met here within this show. Most of the supporting characters aren't fully fleshed characters but rather sad punching bags that want empathy from the audience for being punching bags. As in any sitcom, they are the ones who are made the most normal for the audience to relate to, and in doing this they negate the lead character to such an extent that we see Bittinger being himself and harming people and they just stay there because....why? There is no reason. Any normal people would have simply left the abuse. Keeping them there without any real reason--even the really unbelievable one given by Joanna Cassidy in the very special 2-part abortion episode that has major problems of its own--is where the show just falls apart. To simply believe that people put up with this guy because we are told he has a heart of gold does not mesh with the reality of the situation. If anything, this isn't even dramedy. This is a badly plotted, conceived, and executed premise that had a few moments but overall met the fate it deserved. Someone had the guts to go out and make a very good idea, but the execution is so haphazard that it just looks like a weirdly scripted version of the Jerry Springer show where someone is abused by this tyrant that we're supposed to root for because we are told to. A show like this requires a deft touch that the actors here could have provided easily, but somehow aren't able to. And that's a fatal error that really killed the program.\\n\\n\\n\\nChalk it up to a show in its infancy. Regardless, the show is worth a watch. But it really screwed up when trying to aim for the stars, and made the whole enterprise not what it could have been.\",\n",
       " {'cats': {'pos': False, 'neg': True}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data preprocessed it's time we train our model to classify the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A Classifier\n",
    "\n",
    "We will use a CNN to analyze the sentiments, CNNs can work with other forms of text classification as long as the right data is provided along side the right labels.\n",
    "\n",
    "Lets build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int\n",
    ")-> None:\n",
    "#     built pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\"textcat\", config = {\"architecture\" : \"simple_cnn\"})\n",
    "        nlp.add_pipe(textcat, last = True)\n",
    "        \n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "        \n",
    "    textcat.add_label(\"post\")\n",
    "    textcat.add_label(\"neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples and explanations on the above code visit [](https://spacy.io/usage/examples#textcat). Basicaly what we did is to load the en_core_web_sm module and the check if the pipeline contains the textcat pipeline name if not we create it using .create_pipe)() and then add it to the pipeline at the very end using the \"last\" argument.\n",
    "\n",
    "Incase the component or pipeline exitss we store it in a variable we can use later on in our programm. To do this we use the .get_pipe() to fetch the textcat if it exits. To classify our text we need to tell textcat what it should be looking for in this case its the labels, so we use .add_label() to add it to the textcat.\n",
    "\n",
    "Now we have told textcat what it needs to look for in training so it can be able to learn and hence classify other datasets. Now its time to train the model. Lets begin by writing the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Training loop\n",
    "\n",
    "In building the training loop we want to only train the the textcat component(pipe), hence we first need to exclude the other pipes and disable them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int\n",
    ")-> None:\n",
    "#     built pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\"textcat\", config = {\"architecture\" : \"simple_cnn\"})\n",
    "        nlp.add_pipe(textcat, last = True)\n",
    "        \n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "        \n",
    "    textcat.add_label(\"post\")\n",
    "    textcat.add_label(\"neg\")\n",
    "    \n",
    "    training_excluded_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "    \n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Begining training...\")\n",
    "        batch_size = compounding(4.0, 32.0, 1.001)\n",
    "        \n",
    "    for i in range(iterations):\n",
    "        loss = {}\n",
    "        random.shuffle(training_data)\n",
    "        batches = minibatch(training_data, size = batch_size)\n",
    "        for batch in batches:\n",
    "            text, labels = zip(*batch)\n",
    "            nlp.update(\n",
    "            text, \n",
    "            labels,\n",
    "            drop = 0.2,\n",
    "            sgd = optimizer,\n",
    "            losses = loss\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used nlp.begin_training() which returns an initial optimizer used by np.update() to update the weights of the underlying model.\n",
    "\n",
    "For each iteration we'll create an empty dictionary called loss which will be updated by the nlp.update(). During each iteration we will split the data into minibatches using the minibatch(). \n",
    "\n",
    "For each of the samples in the batch we'll split it into text and labels which we will then pass to the updat() which actaully runs the training on the underlying model.\n",
    "\n",
    "The dropout parameter tells nlp.update() what proportion of the training data in that batch to skip drop during trainging. You do this to make it harder for the model to accidentally just memorize training data without coming up with a generalizable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    tokenizer, textcat, test_data: list\n",
    ") -> dict:\n",
    "    reviews, labels = zip(*test_data)\n",
    "    reviews = (tokenizer(review) for review in reviews)\n",
    "    true_positives = 0\n",
    "    false_positives = 1e-8  # Can't be 0 because of presence in denominator\n",
    "    true_negatives = 0\n",
    "    false_negatives = 1e-8\n",
    "    for i, review in enumerate(textcat.pipe(reviews)):\n",
    "        true_label = labels[i][\"cats\"]\n",
    "        for predicted_label, score in review.cats.items():\n",
    "            # Every cats dictionary includes both labels. You can get all\n",
    "            # the info you need with just the pos label.\n",
    "            if (\n",
    "                predicted_label == \"neg\"\n",
    "            ):\n",
    "                continue\n",
    "#             print(true_label)\n",
    "            if score >= 0.5 and true_label[\"pos\"]:\n",
    "                true_positives += 1\n",
    "            elif score >= 0.5 and true_label[\"neg\"]:\n",
    "                false_positives += 1\n",
    "            elif score < 0.5 and true_label[\"neg\"]:\n",
    "                true_negatives += 1\n",
    "            elif score < 0.5 and true_label[\"pos\"]:\n",
    "                false_negatives += 1\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f_score = 0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f-score\": f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int = 20\n",
    ") -> None:\n",
    "    # Build pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    textcat.add_label(\"pos\")\n",
    "    textcat.add_label(\"neg\")\n",
    "\n",
    "    # Train only textcat\n",
    "    training_excluded_pipes = [\n",
    "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
    "    ]\n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        # Training loop\n",
    "        print(\"Beginning training\")\n",
    "        print(\"Loss\\tPrecision\\tRecall\\tF-score\")\n",
    "        batch_sizes = compounding(\n",
    "            4.0, 32.0, 1.001\n",
    "        )  # A generator that yields infinite series of input numbers\n",
    "        for i in range(iterations):\n",
    "            print(f\"Training iteration {i}\")\n",
    "            loss = {}\n",
    "            random.shuffle(training_data)\n",
    "            batches = minibatch(training_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                text, labels = zip(*batch)\n",
    "                nlp.update(text, labels, drop=0.2, sgd=optimizer, losses=loss)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                evaluation_results = evaluate_model(\n",
    "                    tokenizer=nlp.tokenizer,\n",
    "                    textcat=textcat,\n",
    "                    test_data=test_data\n",
    "                )\n",
    "                print(\n",
    "                    f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
    "                    f\"\\t{evaluation_results['recall']}\"\n",
    "                    f\"\\t{evaluation_results['f-score']}\"\n",
    "                )\n",
    "\n",
    "    # Save model\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(\"model_artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_REVIEW = \"\"\"\n",
    "Transcendently beautiful in moments outside the office, it seems almost\n",
    "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
    "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
    "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
    "doesn't matter. (The worst is sort of tedious - like Office Space with less humor.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(input_data: str = TEST_REVIEW):\n",
    "    #  Load saved trained model\n",
    "    loaded_model = spacy.load(\"model_artifacts\")\n",
    "    # Generate prediction\n",
    "    parsed_text = loaded_model(input_data)\n",
    "    # Determine prediction to return\n",
    "    if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
    "        prediction = \"Positive\"\n",
    "        score = parsed_text.cats[\"pos\"]\n",
    "    else:\n",
    "        prediction = \"Negative\"\n",
    "        score = parsed_text.cats[\"neg\"]\n",
    "    print(\n",
    "        f\"Review text: {input_data}\\nPredicted sentiment: {prediction}\"\n",
    "        f\"\\tScore: {score}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n",
      "Loss\tPrecision\tRecall\tF-score\n",
      "Training iteration 0\n",
      "11.505850926274434\t0.7350746268382434\t0.7635658914432726\t0.7490494296293136\n",
      "Training iteration 1\n",
      "2.0383097512676613\t0.8132780082650092\t0.7596899224511748\t0.7855711422530833\n",
      "Training iteration 2\n",
      "0.5732080414236407\t0.8114754098028084\t0.7674418604353704\t0.7888446214825161\n",
      "Training iteration 3\n",
      "0.1899917852024373\t0.82426778239229\t0.7635658914432726\t0.7927565392035106\n",
      "Training iteration 4\n",
      "0.07318920258921935\t0.8262711864056664\t0.755813953459077\t0.7894736841785638\n",
      "Training iteration 5\n",
      "0.04007941439840579\t0.8305084745410801\t0.7596899224511748\t0.7935222671743513\n",
      "Training iteration 6\n",
      "0.01971181024350699\t0.8382978723047533\t0.7635658914432726\t0.7991886409412092\n",
      "Training iteration 7\n",
      "0.009295679886122343\t0.8347457626764938\t0.7635658914432726\t0.7975708501701388\n",
      "Training iteration 8\n",
      "0.004880438206953386\t0.8262711864056664\t0.755813953459077\t0.7894736841785638\n",
      "Training iteration 9\n",
      "0.002710866429310954\t0.825531914858488\t0.7519379844669791\t0.7870182555461656\n",
      "Training iteration 10\n",
      "0.0017581728340285707\t0.8283261802219603\t0.7480620154748813\t0.7861507127989348\n",
      "Training iteration 11\n",
      "0.0011020767009028987\t0.8290598290243991\t0.7519379844669791\t0.7886178861468042\n",
      "Training iteration 12\n",
      "0.0015418886024320955\t0.8297872340072431\t0.755813953459077\t0.7910750506778469\n",
      "Training iteration 13\n",
      "0.0013220748884634759\t0.8297872340072431\t0.755813953459077\t0.7910750506778469\n",
      "Training iteration 14\n",
      "0.0005863031955755105\t0.8305084745410801\t0.7596899224511748\t0.7935222671743513\n",
      "Training iteration 15\n",
      "0.0005448678047201838\t0.8347457626764938\t0.7635658914432726\t0.7975708501701388\n",
      "Training iteration 16\n",
      "0.0005612457622916622\t0.8382978723047533\t0.7635658914432726\t0.7991886409412092\n",
      "Training iteration 17\n",
      "0.0005213290297376716\t0.8382978723047533\t0.7635658914432726\t0.7991886409412092\n",
      "Training iteration 18\n",
      "0.00037413232461247503\t0.8418803418443641\t0.7635658914432726\t0.800813008097528\n",
      "Training iteration 19\n",
      "0.00038705115479586993\t0.8425531914535084\t0.7674418604353704\t0.8032454360728907\n",
      "Testing model\n",
      "Review text: \n",
      "Transcendently beautiful in moments outside the office, it seems almost\n",
      "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
      "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
      "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
      "doesn't matter. (The worst is sort of tedious - like Office Space with less humor.)\n",
      "\n",
      "Predicted sentiment: Positive\tScore: 0.9998461008071899\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train, test = load_training_data(limit=2500)\n",
    "    train_model(train, test)\n",
    "    print(\"Testing model\")\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: I hate the movie\n",
      "Predicted sentiment: Negative\tScore: 0.9999545812606812\n"
     ]
    }
   ],
   "source": [
    "test_model(input_data = \"I hate the movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: Love the movie\n",
      "Predicted sentiment: Positive\tScore: 0.9999545812606812\n"
     ]
    }
   ],
   "source": [
    "test_model(\"Love the movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: This movie is not fun to watch, it has alot of boring characters, it should be improved\n",
      "Predicted sentiment: Negative\tScore: 0.9998980760574341\n"
     ]
    }
   ],
   "source": [
    "test_model(\"This movie is not fun to watch, it has alot of boring characters, it should be improved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: gaming is boring\n",
      "Predicted sentiment: Negative\tScore: 0.9999545812606812\n"
     ]
    }
   ],
   "source": [
    "test_model(\"gaming is boring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: That leads to very accurate data.\n",
      "Predicted sentiment: Negative\tScore: 0.9999545812606812\n"
     ]
    }
   ],
   "source": [
    "test_model(\"That leads to very accurate data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: (\"I rented this because I'm a bit weary of '80s NBC programming and apparently I saved myself a lot of money. I have nothing against any of the actors and for their credit they do a good job but this show is flawed from the premise.\\n\\n\\n\\nWe have a character who is unlikable. He's full of flaws, not enlightened, and a complete jerk on a good day. Yet the reason why anybody should care just isn't there. While creating an American sitcom centered around a complete bullheaded jackass is revolutionary and full of potential, it just isn't met here within this show. Most of the supporting characters aren't fully fleshed characters but rather sad punching bags that want empathy from the audience for being punching bags. As in any sitcom, they are the ones who are made the most normal for the audience to relate to, and in doing this they negate the lead character to such an extent that we see Bittinger being himself and harming people and they just stay there because....why? There is no reason. Any normal people would have simply left the abuse. Keeping them there without any real reason--even the really unbelievable one given by Joanna Cassidy in the very special 2-part abortion episode that has major problems of its own--is where the show just falls apart. To simply believe that people put up with this guy because we are told he has a heart of gold does not mesh with the reality of the situation. If anything, this isn't even dramedy. This is a badly plotted, conceived, and executed premise that had a few moments but overall met the fate it deserved. Someone had the guts to go out and make a very good idea, but the execution is so haphazard that it just looks like a weirdly scripted version of the Jerry Springer show where someone is abused by this tyrant that we're supposed to root for because we are told to. A show like this requires a deft touch that the actors here could have provided easily, but somehow aren't able to. And that's a fatal error that really killed the program.\\n\\n\\n\\nChalk it up to a show in its infancy. Regardless, the show is worth a watch. But it really screwed up when trying to aim for the stars, and made the whole enterprise not what it could have been.\", {'cats': {'pos': False, 'neg': True}})\n",
      "Predicted sentiment: Negative\tScore: 0.9973537921905518\n"
     ]
    }
   ],
   "source": [
    "test_model(str(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
