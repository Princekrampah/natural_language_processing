{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries to install\n",
    "```\n",
    "$ pip install spacy\n",
    "$ python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "## Whar is sentiment analysis?\n",
    "\n",
    "**Sentiment Analysis** is the process of analysing text with the aim of trying to understand the context of the text or the opinion expressed within a block of text. Its a tool that allows computers to understand the underlying tone in a given text. Sentiment analysis give computers the abilily to understand human text. This can be difficult to achieve since its even difficult for humans to understand text coming to talk of computers it gets even harder. But there are tools and techniques to do that.\n",
    "\n",
    "## Why sentiment analysis\n",
    "\n",
    "1. To understand customers opinion on a certain service or product.\n",
    "\n",
    "2. Useful in social media monitoring\n",
    "\n",
    "3. Used to process data to detect opinions and emotions of a group of people\n",
    "\n",
    "## Objectives of this tutorial\n",
    "\n",
    "1. Learn Natual language processing techniques\n",
    "\n",
    "2. Use ML to determine the sentiment of a text\n",
    "\n",
    "3. How to use spaCy in sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing to Clean data\n",
    "\n",
    "NLP just like any other ML algorithm requiers a well cleaned dataset. The main aim of this is to reduce the noise in the text data that is inherent in human text. There are many tools to do this some of the common ones include Natural language Toolkit, TextBlob  and SpaCy. In this tutorial we will be using spaCy. In NLP we use the following process to clean our data:\n",
    "\n",
    "1. Tokenizing sentences\n",
    "\n",
    "2. Removing stop words like “if,” “but,” “or,” and so on\n",
    "\n",
    "3. Normalizing words\n",
    "\n",
    "4. Vectorizing text\n",
    "\n",
    "\n",
    "\n",
    "## Terminologies\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "This is the technique or process of breaking down a sentence or a chunk of text into smaller pieces. Tokenization is the first step in NLP pipeline. There are mainly two forms or types of tokenization, this include:\n",
    "\n",
    "#### Word Tokenization \n",
    "\n",
    "This type of tokenization involves breaking a text into individual words.\n",
    "\n",
    "#### Sentence Tokenization\n",
    "\n",
    "This involves breaking down a text into a individual of sentences\n",
    "\n",
    "#### Example of word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[,\n",
       " Dave,\n",
       " watched,\n",
       " as,\n",
       " the,\n",
       " forest,\n",
       " burned,\n",
       " up,\n",
       " on,\n",
       " the,\n",
       " hill,\n",
       " ,,\n",
       " ,\n",
       " only,\n",
       " a,\n",
       " few,\n",
       " miles,\n",
       " from,\n",
       " his,\n",
       " house,\n",
       " .,\n",
       " The,\n",
       " car,\n",
       " had,\n",
       " ,\n",
       " been,\n",
       " hastily,\n",
       " packed,\n",
       " and,\n",
       " Marta,\n",
       " was,\n",
       " inside,\n",
       " trying,\n",
       " to,\n",
       " round,\n",
       " ,\n",
       " up,\n",
       " the,\n",
       " last,\n",
       " of,\n",
       " the,\n",
       " pets,\n",
       " .,\n",
       " \",\n",
       " Where,\n",
       " could,\n",
       " she,\n",
       " be,\n",
       " ?,\n",
       " \",\n",
       " he,\n",
       " wondered,\n",
       " ,\n",
       " as,\n",
       " he,\n",
       " continued,\n",
       " to,\n",
       " wait,\n",
       " for,\n",
       " Marta,\n",
       " to,\n",
       " appear,\n",
       " with,\n",
       " the,\n",
       " pets,\n",
       " .,\n",
       " ]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "text = \"\"\"\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets.\n",
    "\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "token_list = [token for token in doc]\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Word\n",
    "\n",
    "stop words are characters that might be useful in human communication but are not important in sentiment analysis in a machine learning model or a deep learning model. We need to remove these words example \"the\", \"as\"...\n",
    "\n",
    "SpaCy come with a defual list of this stop words that we can use to remove stop words from the tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[,\n",
       " Dave,\n",
       " watched,\n",
       " forest,\n",
       " burned,\n",
       " hill,\n",
       " ,,\n",
       " ,\n",
       " miles,\n",
       " house,\n",
       " .,\n",
       " car,\n",
       " ,\n",
       " hastily,\n",
       " packed,\n",
       " Marta,\n",
       " inside,\n",
       " trying,\n",
       " round,\n",
       " ,\n",
       " pets,\n",
       " .,\n",
       " \",\n",
       " ?,\n",
       " \",\n",
       " wondered,\n",
       " ,\n",
       " continued,\n",
       " wait,\n",
       " Marta,\n",
       " appear,\n",
       " pets,\n",
       " .,\n",
       " ]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Now that we have removed the unnecessary word, lets normalize the remaining text. Normalization is the process of condensing many variations of a single word into one single variation. Example take the words \"wait\", \"waited\" and \"waiting\" will be condensed into \"wait\".\n",
    "\n",
    "#### Types of Normalization\n",
    "\n",
    "There are two main types of normalization:\n",
    "\n",
    "1. Stemming\n",
    "\n",
    "With this method, a word is trimmed off at its stem, like in the two examples we looked at earlier. Example \"wait\", \"waiting\" and \"waited\" have a common base of \"wait\". The relationship between \"drive\" and \"drove\" will be missed.\n",
    "\n",
    "2. Lemmatization\n",
    "\n",
    "This is a more powerful form of stemming, Lemmatization considers the context and converts the word to its base meaningful form which is called lemma.\n",
    "\n",
    "Lemmatization can also be said as, the process of converting the given word into it's base form according to the dictionary meaning of the word.\n",
    "\n",
    "Example: \"studying\" and \"studies\" all have the dictionary or the meaninful meaning of \"study\"\n",
    "\n",
    "Since lemmatization is more powerful and efficient than stemming spaCy only provides lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token: \\n, lemma: \\n',\n",
       " 'token: Dave, lemma: Dave',\n",
       " 'token: watched, lemma: watch',\n",
       " 'token: forest, lemma: forest',\n",
       " 'token: burned, lemma: burn',\n",
       " 'token: hill, lemma: hill',\n",
       " 'token: ,, lemma: ,',\n",
       " 'token: \\n, lemma: \\n',\n",
       " 'token: miles, lemma: mile',\n",
       " 'token: house, lemma: house',\n",
       " 'token: ., lemma: .',\n",
       " 'token: car, lemma: car',\n",
       " 'token: \\n, lemma: \\n',\n",
       " 'token: hastily, lemma: hastily',\n",
       " 'token: packed, lemma: pack',\n",
       " 'token: Marta, lemma: Marta',\n",
       " 'token: inside, lemma: inside',\n",
       " 'token: trying, lemma: try',\n",
       " 'token: round, lemma: round',\n",
       " 'token: \\n, lemma: \\n',\n",
       " 'token: pets, lemma: pet',\n",
       " 'token: ., lemma: .',\n",
       " 'token: \", lemma: \"',\n",
       " 'token: ?, lemma: ?',\n",
       " 'token: \", lemma: \"',\n",
       " 'token: wondered, lemma: wonder',\n",
       " 'token: \\n, lemma: \\n',\n",
       " 'token: continued, lemma: continue',\n",
       " 'token: wait, lemma: wait',\n",
       " 'token: Marta, lemma: Marta',\n",
       " 'token: appear, lemma: appear',\n",
       " 'token: pets, lemma: pet',\n",
       " 'token: ., lemma: .',\n",
       " 'token: \\n, lemma: \\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma = [f\"token: {token}, lemma: {token.lemma_}\" for token in filtered_tokens]\n",
    "lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Vectorization is the process of converting the tokenized words into a numeric array of numbers which is unique to a given token and  represents various features of a token. Vecotrs are used to find the similarities within words, classify text and perform other NLP operations.\n",
    "\n",
    "The arrays in that represent a vector for a given token can either be **densed array** in which very space in the array contains a defined value. On the other hand **sparsed array** most of the spaces in the array is empty. In most cases we'll use dense array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dave"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8371642 ,  1.452925  , -1.6147203 ,  0.67836225, -0.659443  ,\n",
       "        1.6417911 ,  0.57964015,  2.3021278 , -0.13260579,  0.57509375,\n",
       "        1.5654867 , -0.69388777, -0.5960694 , -1.5377433 ,  1.9425607 ,\n",
       "       -2.4552503 ,  1.2321602 ,  1.0434954 , -1.5102386 , -0.57876253,\n",
       "        0.12055516,  3.6501799 ,  2.616098  , -0.57102156, -1.5221778 ,\n",
       "        0.0062914 ,  0.22760749, -1.9220744 , -1.6252842 , -4.2262235 ,\n",
       "       -3.495663  , -3.3120532 ,  0.81387675, -0.00677478, -0.11603296,\n",
       "        1.462044  ,  3.0751472 ,  0.35958475, -0.22526968, -2.7439258 ,\n",
       "        1.2696334 ,  4.606787  ,  0.3403422 , -2.127231  ,  1.261918  ,\n",
       "       -4.209798  ,  5.4528546 ,  1.6940243 , -2.597298  ,  0.95049405,\n",
       "       -1.9105787 , -2.374928  , -1.4227569 , -2.2528832 , -1.7998077 ,\n",
       "        1.607501  ,  2.9914231 ,  2.8065157 , -1.2510273 , -0.5496425 ,\n",
       "       -0.49980426, -1.3882611 , -0.47047865, -2.9670255 ,  1.7884939 ,\n",
       "        4.5282784 , -1.2602415 , -0.14885461,  1.0419188 , -0.08892578,\n",
       "       -1.1382759 ,  2.2426186 ,  1.5077226 , -1.5030198 ,  2.5280986 ,\n",
       "       -1.6761327 ,  0.16694829,  2.12396   ,  0.02546155,  0.3875456 ,\n",
       "        0.89119613, -0.07678461, -2.0690749 , -1.121186  ,  1.4821008 ,\n",
       "        1.1989202 ,  2.1933234 ,  0.52963793,  3.0646472 , -1.722329  ,\n",
       "       -1.3634226 , -0.47471   , -1.7648505 ,  3.5651777 , -2.3942056 ,\n",
       "       -1.3800393 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[1].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[1].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy For Text Classification\n",
    "\n",
    "SpaCy has alot of this inbuilt functions to preprocess data, it also provides alot of pipeline functions to enable us to label our data.The default pipeline is defined in a JSON file associated with whichever preexisting model you’re using (en_core_web_sm for this tutorial), but you can also build one from scratch if you wish.\n",
    "\n",
    "One of the built-in pipeline functions that spaCy provides is called textcat (TextCategorizer), which enables you to assign categories(labels) to your text data and use that as training data for a neural network or a machine learning model.\n",
    "\n",
    "This process will generate a trained model that you can then use to predict the sentiment of a given piece of text.\n",
    "\n",
    "To achieve this we need to follow the following steps:\n",
    "\n",
    "1. Add the textcat component to the existing pipeline.\n",
    "2. Add valid labels to the textcat component.\n",
    "3. Load, shuffle, and split your data.\n",
    "4. Train the model, evaluating on each training loop.\n",
    "5. Use the trained model to predict the sentiment of non-training data.\n",
    "6. Optionally, save the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Sentiment Analysis\n",
    "\n",
    "To build a sentiment analysis pipline you need to follow the following steps or include the following in your pipelne.\n",
    "\n",
    "1. Loading data\n",
    "2. Preprocessing\n",
    "3. Training the classifier\n",
    "4. Classifying data\n",
    "\n",
    "We will build a movie sentiment analyzer, the dataset well be using is [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "## Loading data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def load_training_data(\n",
    "    data_directory: str = \"./datasets/aclImdb_v1/aclImdb/train\",\n",
    "    split: float = 0.8,\n",
    "    limit: int = 0\n",
    ") -> tuple:\n",
    "     # Load from files\n",
    "    reviews = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\") as f:\n",
    "                    text = f.read()\n",
    "                    text = text.replace(\"<br />\", \"\\n\\n\")\n",
    "                    if text.strip():\n",
    "                        spacy_label = {\n",
    "                            \"cats\": {\n",
    "                                \"pos\": \"pos\" == label,\n",
    "                                \"neg\": \"neg\" == label\n",
    "                            }\n",
    "                        }\n",
    "                        reviews.append((text, spacy_label))\n",
    "    random.shuffle(reviews)\n",
    "\n",
    "    if limit:\n",
    "        reviews = reviews[:limit]\n",
    "    split = int(len(reviews) * split)\n",
    "    return reviews[:split], reviews[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = load_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been able to load the datasets and do some simple processing on the data such as assigning labels to the text we loaded. Now we have a dataset containing labled text. We have also shuffled the data at this point to ensure that ordering does not affect the model accuracy in anyway and also to get the dataset well mixed up before we split the data into training and testing sets.\n",
    "\n",
    "To split the data we use list slicing to do that, we obtain the value used to slice the list by using the argument passed into the function and t multiply it with the total lenght of the reviews list. We return the first 80% of that value and the rest 80% and above.\n",
    "\n",
    "#### NOTE:\n",
    "\n",
    "The label dictionary structure is a format required by the spaCy model during the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"....Rather well done, actually--attack the evil villains in their lair, stop a Little Big Horn style ambush, save the day via the waterboys' bugling, works for me. Stiff Upper British lip and all that.\\n\\n\\n\\nSo how does it play on a DVD 66 years later? Struck me as being like a Western, subbing Apaches or Sioux for the Thugs, and the US Cavalry for the Imperial British Army. It's very Colonial in it's outlook, you know? White Man's burden and all that? Kipling certainly would have approved.\\n\\n\\n\\nCary Grant, Fairbanks and MacLaglen play it as broadly as possible, putting some buddy buddy slapstick into the mix between the shootings and brawlings for good measure. (I had no idea it was Joan Fontaine as the token army wife--did they leave some of her scenes on the cutting room floor? very short-) None of them were aiming for an Oscar here--in fact Grant was not at his best in a few scenes--but sod it, it still works. And where else would Ben Casey wind up as an Indian bugler? Only in Hollywood.\\n\\n\\n\\nDef. check this out if you like adventure and pseudo-Western style antics. It was done by a bunch of pros, well I might add.\\n\\n\\n\\n*** outta ****\",\n",
       "  {'cats': {'pos': True, 'neg': False}}),\n",
       " ('There is good. There is bad. And then their is The Sentinel, a bottom-barrel political \"thriller\" that ranks among the worst movies I have ever seen. The plot of a mole in the Secret Service is a good one, but never has a movie with so much potential been so utterly butchered. Directed with ham-handed \"edginess\" by Clark Johnson, every actor in this film seems to be working on autopilot. Even the great Michael Douglas looks bored here. I can honestly say I have NEVER, in all my life, viewed another film with so many glaring plot holes. The twist is predictable from square one, and the character\\'s motives are so utterly ridiculous that they inspired laughter from the audience. Avoid this at all costs. This is a catastrophe of a movie with no redeeming value.',\n",
       "  {'cats': {'pos': False, 'neg': True}})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1: 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data preprocessed it's time we train our model to classify the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A Classifier\n",
    "\n",
    "We will use a CNN to analyze the sentiments, CNNs can work with other forms of text classification as long as the right data is provided along side the right labels.\n",
    "\n",
    "Lets build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int\n",
    ")-> None:\n",
    "#     built pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\"textcat\", config = {\"architecture\" : \"simple_cnn\"})\n",
    "        nlp.add_pipe(textcat, last = True)\n",
    "        \n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "        \n",
    "    textcat.add_label(\"post\")\n",
    "    textcat.add_label(\"neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples and explanations on the above code visit [](https://spacy.io/usage/examples#textcat). Basicaly what we did is to load the en_core_web_sm module and the check if the pipeline contains the textcat pipeline name if not we create it using .create_pipe)() and then add it to the pipeline at the very end using the \"last\" argument.\n",
    "\n",
    "Incase the component or pipeline exitss we store it in a variable we can use later on in our programm. To do this we use the .get_pipe() to fetch the textcat if it exits. To classify our text we need to tell textcat what it should be looking for in this case its the labels, so we use .add_label() to add it to the textcat.\n",
    "\n",
    "Now we have told textcat what it needs to look for in training so it can be able to learn and hence classify other datasets. Now its time to train the model. Lets begin by writing the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Training loop\n",
    "\n",
    "In building the training loop we want to only train the the textcat component(pipe), hence we first need to exclude the other pipes and disable them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int\n",
    ")-> None:\n",
    "#     built pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\"textcat\", config = {\"architecture\" : \"simple_cnn\"})\n",
    "        nlp.add_pipe(textcat, last = True)\n",
    "        \n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "        \n",
    "    textcat.add_label(\"post\")\n",
    "    textcat.add_label(\"neg\")\n",
    "    \n",
    "    training_excluded_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "    \n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Begining training...\")\n",
    "        batch_size = compounding(4.0, 32.0, 1.001)\n",
    "        \n",
    "    for i in range(iterations):\n",
    "        loss = {}\n",
    "        random.shuffle(training_data)\n",
    "        batches = minibatch(training_data, size = batch_size)\n",
    "        for batch in batches:\n",
    "            text, labels = zip(*batch)\n",
    "            nlp.update(\n",
    "            text, \n",
    "            labels,\n",
    "            drop = 0.2,\n",
    "            sgd = optimizer,\n",
    "            losses = loss\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used nlp.begin_training() which returns an initial optimizer used by np.update() to update the weights of the underlying model.\n",
    "\n",
    "For each iteration we'll create an empty dictionary called loss which will be updated by the nlp.update(). During each iteration we will split the data into minibatches using the minibatch(). \n",
    "\n",
    "For each of the samples in the batch we'll split it into text and labels which we will then pass to the updat() which actaully runs the training on the underlying model.\n",
    "\n",
    "The dropout parameter tells nlp.update() what proportion of the training data in that batch to skip drop during trainging. You do this to make it harder for the model to accidentally just memorize training data without coming up with a generalizable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    tokenizer, textcat, test_data: list\n",
    ") -> dict:\n",
    "    reviews, labels = zip(*test_data)\n",
    "    reviews = (tokenizer(review) for review in reviews)\n",
    "    true_positives = 0\n",
    "    false_positives = 1e-8  # Can't be 0 because of presence in denominator\n",
    "    true_negatives = 0\n",
    "    false_negatives = 1e-8\n",
    "    for i, review in enumerate(textcat.pipe(reviews)):\n",
    "        true_label = labels[i][\"cats\"]\n",
    "        for predicted_label, score in review.cats.items():\n",
    "            # Every cats dictionary includes both labels. You can get all\n",
    "            # the info you need with just the pos label.\n",
    "            if (\n",
    "                predicted_label == \"neg\"\n",
    "            ):\n",
    "                continue\n",
    "#             print(true_label)\n",
    "            if score >= 0.5 and true_label[\"pos\"]:\n",
    "                true_positives += 1\n",
    "            elif score >= 0.5 and true_label[\"neg\"]:\n",
    "                false_positives += 1\n",
    "            elif score < 0.5 and true_label[\"neg\"]:\n",
    "                true_negatives += 1\n",
    "            elif score < 0.5 and true_label[\"pos\"]:\n",
    "                false_negatives += 1\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f_score = 0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f-score\": f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "import spacy\n",
    "\n",
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int = 2\n",
    ")-> None:\n",
    "#     built pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\"textcat\", config = {\"architecture\" : \"simple_cnn\"})\n",
    "        nlp.add_pipe(textcat, last = True)\n",
    "        \n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "        \n",
    "    textcat.add_label(\"post\")\n",
    "    textcat.add_label(\"neg\")\n",
    "    \n",
    "    training_excluded_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "    \n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Begining training...\")\n",
    "        batch_size = compounding(4.0, 32.0, 1.001)\n",
    "        \n",
    "    for i in range(iterations):\n",
    "        loss = {}\n",
    "        random.shuffle(training_data)\n",
    "        batches = minibatch(training_data, size = batch_size)\n",
    "        for batch in batches:\n",
    "            text, labels = zip(*batch)\n",
    "            nlp.update(\n",
    "            text, \n",
    "            labels,\n",
    "            drop = 0.2,\n",
    "            sgd = optimizer,\n",
    "            losses = loss\n",
    "            )\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                evaluation_results = evaluate_model(\n",
    "                tokenizer=nlp.tokenizer,\n",
    "                textcat=textcat,\n",
    "                test_data=test_data\n",
    "                )\n",
    "            print(\n",
    "                f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
    "                f\"\\t{evaluation_results['recall']}\"\n",
    "                f\"\\t{evaluation_results['f-score']}\"\n",
    "            )\n",
    "    #save model\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(\"model_artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_REVIEW = \"\"\"\n",
    "Transcendently beautiful in moments outside the office, it seems almost\n",
    "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
    "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
    "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
    "doesn't matter. (The worst is sort of tedious - like Office Space with less humor.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(input_data: str = TEST_REVIEW):\n",
    "    #  Load saved trained model\n",
    "    loaded_model = spacy.load(\"model_artifacts\")\n",
    "    # Generate prediction\n",
    "    parsed_text = loaded_model(input_data)\n",
    "    # Determine prediction to return\n",
    "    if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
    "        prediction = \"Positive\"\n",
    "        score = parsed_text.cats[\"pos\"]\n",
    "    else:\n",
    "        prediction = \"Negative\"\n",
    "        score = parsed_text.cats[\"neg\"]\n",
    "    print(\n",
    "        f\"Review text: {input_data}\\nPredicted sentiment: {prediction}\"\n",
    "        f\"\\tScore: {score}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining training...\n",
      "0.01833350583910942\t0.4945533768955435\t0.9227642276047656\t0.6439716311874051\n",
      "0.03063676320016384\t0.4866529774027381\t0.9634146341071782\t0.646657571605821\n",
      "0.048128943890333176\t0.48991935482883225\t0.9878048780086258\t0.6549865228933966\n",
      "0.0643150620162487\t0.4899598393475912\t0.9918699186588671\t0.6559139784769915\n",
      "0.09003099985420704\t0.4899598393475912\t0.9918699186588671\t0.6559139784769915\n",
      "0.1068252231925726\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.11731049604713917\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.13455991074442863\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.15015246625989676\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.18378462549299002\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.1953675001859665\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.21018798276782036\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.22461166232824326\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.24794880859553814\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.2630806127563119\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.278623572550714\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.29559429455548525\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.3072788752615452\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.3272412642836571\t0.49199999999016\t0.9999999999593495\t0.6595174262557769\n",
      "0.3526258338242769\t0.48893360159982024\t0.9878048780086258\t0.6541049797939676\n",
      "0.3649043785408139\t0.48893360159982024\t0.9878048780086258\t0.6541049797939676\n",
      "0.38707385305315256\t0.49287169041766044\t0.9837398373583846\t0.6567164178926265\n",
      "0.40312611777335405\t0.49287169041766044\t0.9837398373583846\t0.6567164178926265\n",
      "0.4164314018562436\t0.49183673468384004\t0.9796747967081433\t0.6548913043300301\n",
      "0.4484019624069333\t0.49287169041766044\t0.9837398373583846\t0.6567164178926265\n",
      "0.4619535570964217\t0.49087221094339\t0.9837398373583846\t0.6549391068834929\n",
      "0.48353012558072805\t0.49090909089917356\t0.9878048780086258\t0.6558704453264274\n",
      "0.4972211569547653\t0.48893360159982024\t0.9878048780086258\t0.6541049797939676\n",
      "0.5108873713761568\t0.4899598393475912\t0.9918699186588671\t0.6559139784769915\n",
      "0.5255294824019074\t0.4899598393475912\t0.9918699186588671\t0.6559139784769915\n",
      "0.5527907507494092\t0.4909819639180164\t0.9959349593091082\t0.6577181207877122\n",
      "0.5936282286420465\t0.4909819639180164\t0.9959349593091082\t0.6577181207877122\n",
      "0.6080486075952649\t0.4909819639180164\t0.9959349593091082\t0.6577181207877122\n",
      "0.6560500198975205\t0.4899598393475912\t0.9918699186588671\t0.6559139784769915\n",
      "0.6674810945987701\t0.4909456740343874\t0.9918699186588671\t0.6567967698342719\n",
      "0.6780189583078027\t0.4919354838610497\t0.9918699186588671\t0.6576819406830813\n",
      "0.6876208325847983\t0.48865979380435753\t0.9634146341071782\t0.6484268125677586\n",
      "0.7146325735375285\t0.48214285713209504\t0.8780487804521118\t0.6224783861492081\n",
      "0.735268703661859\t0.4819587628741763\t0.7601626015951153\t0.5899053627574161\n",
      "0.7540929345414042\t0.4440433212836085\t0.49999999997967476\t0.47036328870094213\n",
      "0.7761748684570193\t0.4248704662992295\t0.3333333333197832\t0.3735763097779692\n",
      "0.7928680190816522\t0.4436090225230369\t0.23983739836423423\t0.31134564642156487\n",
      "0.8155809817835689\t0.4787234042043911\t0.18292682926085663\t0.26470588233737025\n",
      "0.8240661323070526\t0.4558823528741349\t0.12601626015747902\t0.1974522292867865\n",
      "0.8379070814698935\t0.42857142850340135\t0.10975609755651397\t0.17475728154208692\n",
      "0.8564142864197493\t0.4237288134875036\t0.10162601625603146\t0.1639344262187584\n",
      "0.8969881441444159\t0.4693877550062474\t0.09349593495554895\t0.15593220337925884\n",
      "0.9127960056066513\t0.46666666656296296\t0.08536585365506642\t0.14432989689729692\n",
      "0.9434646759182215\t0.5151515149954086\t0.06910569105410139\t0.12186379927441834\n",
      "0.9563853442668915\t0.5172413791319858\t0.060975609753618876\t0.10909090908297521\n",
      "0.9767017289996147\t0.47826086935727785\t0.04471544715265384\t0.08178438661101974\n",
      "0.994715541601181\t0.47826086935727785\t0.04471544715265384\t0.08178438661101974\n",
      "1.0083845853805542\t0.4736842102770083\t0.03658536585217133\t0.0679245282967604\n",
      "1.0174623252823949\t0.4999999997222222\t0.03658536585217133\t0.06818181817665289\n",
      "1.0359321599826217\t0.46666666635555554\t0.02845528455168881\t0.053639846739184684\n",
      "1.0500264884904027\t0.46666666635555554\t0.02845528455168881\t0.053639846739184684\n",
      "1.064708256162703\t0.3846153843195266\t0.020325203251206292\t0.03861003860705714\n",
      "1.0783226089552045\t0.3846153843195266\t0.020325203251206292\t0.03861003860705714\n",
      "1.085263119544834\t0.46666666635555554\t0.02845528455168881\t0.053639846739184684\n",
      "1.0976418717764318\t0.4999999996875\t0.032520325201930066\t0.0610687022854146\n",
      "1.106900169979781\t0.5238095235600907\t0.04471544715265384\t0.08239700373914628\n",
      "1.1284659351222217\t0.4999999998076923\t0.05284552845313636\t0.09558823528708908\n",
      "1.1552274241112173\t0.4666666665111111\t0.05691056910337762\t0.10144927535496744\n",
      "1.1671462724916637\t0.4285714284489796\t0.060975609753618876\t0.10676156582870024\n",
      "1.184038947802037\t0.41666666655092593\t0.060975609753618876\t0.10638297871585936\n",
      "1.2081557321362197\t0.4117647057612457\t0.05691056910337762\t0.09999999999285715\n",
      "1.2206284892745316\t0.39393939382001836\t0.05284552845313636\t0.09318996415102582\n",
      "1.240624996367842\t0.3749999998828125\t0.0487804878028951\t0.0863309352455877\n",
      "1.2781515368260443\t0.3999999998666667\t0.0487804878028951\t0.08695652173282924\n",
      "1.312654493842274\t0.3999999998666667\t0.0487804878028951\t0.08695652173282924\n",
      "1.3334879488684237\t0.371428571322449\t0.05284552845313636\t0.09252669038487354\n",
      "1.3554637343622744\t0.42222222212839505\t0.07723577235458391\t0.1305841924308877\n",
      "1.3674753517843783\t0.449999999925\t0.10975609755651397\t0.17647058822376008\n",
      "1.3797981250099838\t0.39999999994999996\t0.13008130080772026\t0.19631901839286386\n",
      "1.389127207454294\t0.38461538457311917\t0.14227642275844404\t0.20771513351882998\n",
      "1.4019469036720693\t0.35652173909943286\t0.1666666666598916\t0.2271468143918478\n",
      "1.4125102958641946\t0.36718749997131345\t0.19105691056133914\t0.2513368983822814\n",
      "1.429133006837219\t0.3669064747937477\t0.2073170731623042\t0.26493506492130203\n",
      "1.442333907354623\t0.38124999997617187\t0.24796747966471677\t0.3004926108226358\n",
      "1.4651855421252549\t0.3801169590420984\t0.2642276422656818\t0.3117505995054316\n",
      "1.479811844881624\t0.3714285714073469\t0.2642276422656818\t0.30878859856015256\n",
      "1.4903100938536227\t0.3795180722662941\t0.25609756096519926\t0.30582524270360073\n",
      "1.4987304392270744\t0.37735849054230447\t0.2439024390144755\t0.29629629628166443\n",
      "1.5127123747952282\t0.35135135132761136\t0.21138211381254543\t0.2639593908495452\n",
      "1.5275465263985097\t0.35766423355053545\t0.19918699186182165\t0.25587467361588123\n",
      "1.543368463870138\t0.3609022556119622\t0.1951219512115804\t0.25329815302093417\n",
      "1.5662004849873483\t0.37692307689408283\t0.19918699186182165\t0.2606382978584767\n",
      "1.5859443279914558\t0.3709677419055671\t0.1869918699110979\t0.24864864863520816\n",
      "1.6110293972305954\t0.35576923073502215\t0.15040650405892655\t0.21142857141648974\n",
      "1.628501224797219\t0.3199999999573333\t0.0975609756057902\t0.14953271027105716\n",
      "1.6466478924266994\t0.319999999936\t0.06504065040386013\t0.1081081081008035\n",
      "1.6628186809830368\t0.3749999998828125\t0.0487804878028951\t0.0863309352455877\n",
      "1.6656226806808263\t0.23809523798185941\t0.020325203251206292\t0.03745318351779377\n",
      "1.678919147932902\t0.07692307686390532\t0.004065040650241258\t0.0077220077214114275\n",
      "1.6901786441449076\t0.0\t0.0\t0\n",
      "1.7192068214062601\t0.0\t0.0\t0\n",
      "1.7294437403324991\t0.0\t0.0\t0\n",
      "1.7440212781075388\t0.0\t0.0\t0\n",
      "1.7739699131343514\t0.0\t0.0\t0\n",
      "1.7890598981175572\t0.0\t0.0\t0\n",
      "1.8026472080964595\t0.0\t0.0\t0\n",
      "1.8033271651365794\t0.0\t0.0\t0\n",
      "1.82777381275082\t0.0\t0.0\t0\n",
      "1.8552945177652873\t0.0\t0.0\t0\n",
      "1.8789185542264022\t0.0\t0.0\t0\n",
      "1.9010590068646707\t0.0\t0.0\t0\n",
      "1.9240340120741166\t0.0\t0.0\t0\n",
      "1.9462169181206264\t0.0\t0.0\t0\n",
      "1.9584692116477527\t0.0\t0.0\t0\n",
      "1.970762595126871\t0.0\t0.0\t0\n",
      "1.981426484475378\t0.0\t0.0\t0\n",
      "1.991346858034376\t0.0\t0.0\t0\n",
      "2.010683245549444\t0.0\t0.0\t0\n",
      "2.0286011971184053\t0.0\t0.0\t0\n",
      "2.04574421007419\t0.0\t0.0\t0\n",
      "2.059576537052635\t0.0\t0.0\t0\n",
      "2.0782929654815234\t0.0\t0.0\t0\n",
      "2.096489436517004\t0.0\t0.0\t0\n",
      "2.107025343517307\t0.0\t0.0\t0\n",
      "2.124762342835311\t0.0\t0.0\t0\n",
      "2.1400309871532954\t0.0\t0.0\t0\n",
      "2.155978831986431\t0.0\t0.0\t0\n",
      "2.164920261653606\t0.0\t0.0\t0\n",
      "2.183781587111298\t0.0\t0.0\t0\n",
      "2.2006285940879025\t0.0\t0.0\t0\n",
      "2.2162171886884607\t0.0\t0.0\t0\n",
      "2.2209608196862973\t0.0\t0.0\t0\n",
      "2.2362729129963554\t0.0\t0.0\t0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2514147083857097\t0.0\t0.0\t0\n",
      "2.25557733658934\t0.0\t0.0\t0\n",
      "2.269516665081028\t0.0\t0.0\t0\n",
      "2.285760143364314\t0.0\t0.0\t0\n",
      "2.2987815826782025\t0.0\t0.0\t0\n",
      "2.3232580512412824\t0.0\t0.0\t0\n",
      "2.339317867124919\t0.0\t0.0\t0\n",
      "2.3591455775895156\t0.0\t0.0\t0\n",
      "2.3747200160869397\t0.0\t0.0\t0\n",
      "2.3855428775423206\t0.0\t0.0\t0\n",
      "2.394990338536445\t0.0\t0.0\t0\n",
      "2.4033229495980777\t0.0\t0.0\t0\n",
      "2.4238171014585532\t0.0\t0.0\t0\n",
      "2.432139793934766\t0.0\t0.0\t0\n",
      "2.4550320380949415\t0.0\t0.0\t0\n",
      "2.4662782983505167\t0.0\t0.0\t0\n",
      "2.4793608752661385\t0.0\t0.0\t0\n",
      "2.497911874728743\t0.0\t0.0\t0\n",
      "2.510267506877426\t0.0\t0.0\t0\n",
      "2.520463876950089\t0.0\t0.0\t0\n",
      "2.54913007147843\t0.0\t0.0\t0\n",
      "2.552994033379946\t0.0\t0.0\t0\n",
      "2.564883984683547\t0.0\t0.0\t0\n",
      "2.585681367723737\t0.0\t0.0\t0\n",
      "2.599059004394803\t0.0\t0.0\t0\n",
      "2.630183821602259\t0.0\t0.0\t0\n",
      "2.639787144085858\t0.0\t0.0\t0\n",
      "2.652489545813296\t0.0\t0.0\t0\n",
      "2.6771099111647345\t0.0\t0.0\t0\n",
      "2.689322116842959\t0.0\t0.0\t0\n",
      "2.6994565697968937\t0.0\t0.0\t0\n",
      "2.70829641452292\t0.0\t0.0\t0\n",
      "2.720529811515007\t0.0\t0.0\t0\n",
      "2.731046700908337\t0.0\t0.0\t0\n",
      "2.741856040491257\t0.0\t0.0\t0\n",
      "2.748013622185681\t0.0\t0.0\t0\n",
      "2.762713842152152\t0.0\t0.0\t0\n",
      "2.781802054552827\t0.0\t0.0\t0\n",
      "2.788914395554457\t0.0\t0.0\t0\n",
      "2.797768950520549\t0.0\t0.0\t0\n",
      "2.8066443652496673\t0.0\t0.0\t0\n",
      "2.821351349411998\t0.0\t0.0\t0\n",
      "2.8338818187476136\t0.0\t0.0\t0\n",
      "2.841454216337297\t0.0\t0.0\t0\n",
      "2.8583114101202227\t0.0\t0.0\t0\n",
      "2.873912022507284\t0.0\t0.0\t0\n",
      "2.886238492152188\t0.0\t0.0\t0\n",
      "2.9071566565544344\t0.0\t0.0\t0\n",
      "2.922556893026922\t0.0\t0.0\t0\n",
      "2.937399791611824\t0.0\t0.0\t0\n",
      "2.9406703946297057\t0.0\t0.0\t0\n",
      "2.9499765022774227\t0.0\t0.0\t0\n",
      "2.9675340674002655\t0.0\t0.0\t0\n",
      "2.9821431025047787\t0.0\t0.0\t0\n",
      "3.008384393819142\t0.0\t0.0\t0\n",
      "3.0263946469058283\t0.0\t0.0\t0\n",
      "3.0503242856939323\t0.0\t0.0\t0\n",
      "3.061073054850567\t0.0\t0.0\t0\n",
      "3.0840350103680976\t0.0\t0.0\t0\n",
      "3.107786654785741\t0.0\t0.0\t0\n",
      "3.119138399430085\t0.0\t0.0\t0\n",
      "3.1320097710122354\t0.0\t0.0\t0\n",
      "3.1481496024061926\t0.0\t0.0\t0\n",
      "3.1592679445748217\t0.0\t0.0\t0\n",
      "3.179327954829205\t0.0\t0.0\t0\n",
      "3.1893436296959408\t0.0\t0.0\t0\n",
      "3.1987038946826942\t0.0\t0.0\t0\n",
      "3.205798034381587\t0.0\t0.0\t0\n",
      "3.226102144282777\t0.0\t0.0\t0\n",
      "3.234737922262866\t0.0\t0.0\t0\n",
      "3.2519324299064465\t0.0\t0.0\t0\n",
      "3.2646891808253713\t0.0\t0.0\t0\n",
      "3.2764398640138097\t0.0\t0.0\t0\n",
      "3.2902871647966094\t0.0\t0.0\t0\n",
      "3.306206999171991\t0.09090909082644627\t0.004065040650241258\t0.007782101166709563\n",
      "3.323487128887791\t0.07692307686390532\t0.004065040650241258\t0.0077220077214114275\n",
      "3.3442732224357314\t0.07692307686390532\t0.004065040650241258\t0.0077220077214114275\n",
      "3.3575743779656477\t0.06666666662222222\t0.004065040650241258\t0.007662835248454955\n",
      "3.3863525621709414\t0.124999999921875\t0.008130081300482516\t0.01526717557135365\n",
      "3.3986734550562687\t0.15789473675900276\t0.012195121950723775\t0.02264150943225347\n",
      "3.4038360750419088\t0.1999999999\t0.016260162600965033\t0.03007518796766352\n",
      "3.4174947643768974\t0.23809523798185941\t0.020325203251206292\t0.03745318351779377\n",
      "3.427594793785829\t0.21739130425330813\t0.020325203251206292\t0.03717472118682716\n",
      "3.4331279125181027\t0.21739130425330813\t0.020325203251206292\t0.03717472118682716\n",
      "3.463948064309079\t0.20833333324652778\t0.020325203251206292\t0.037037037034293556\n",
      "3.4711685877409764\t0.1739130434026465\t0.016260162600965033\t0.029739776949461727\n",
      "3.4805166170117445\t0.1739130434026465\t0.016260162600965033\t0.029739776949461727\n",
      "3.5022312391665764\t0.1739130434026465\t0.016260162600965033\t0.029739776949461727\n",
      "3.5232228551176377\t0.1666666665972222\t0.016260162600965033\t0.029629629627434845\n",
      "3.538972475391347\t0.159999999936\t0.016260162600965033\t0.02952029520077341\n",
      "3.551596208300907\t0.19230769223372782\t0.020325203251206292\t0.03676470587964965\n",
      "3.5678287286427803\t0.2580645160457856\t0.032520325201930066\t0.05776173284781504\n",
      "3.577986417163629\t0.2571428570693878\t0.03658536585217133\t0.06405693949722016\n",
      "3.5933794507873245\t0.23529411757785468\t0.032520325201930066\t0.05714285713877551\n",
      "3.5992785522830673\t0.24242424235078053\t0.032520325201930066\t0.05734767024678511\n",
      "3.6087421186384745\t0.2727272726446281\t0.03658536585217133\t0.06451612902763326\n",
      "3.6157855166238733\t0.2727272726446281\t0.03658536585217133\t0.06451612902763326\n",
      "3.628759833809454\t0.24137931026159334\t0.02845528455168881\t0.05090909090538843\n",
      "3.650988363486249\t0.22222222213991769\t0.02439024390144755\t0.04395604395282373\n",
      "3.6554081070353277\t0.19999999992\t0.020325203251206292\t0.03690036900096676\n",
      "3.6676575227756985\t0.19999999992\t0.020325203251206292\t0.03690036900096676\n",
      "3.682003086723853\t0.23076923068047336\t0.02439024390144755\t0.044117647055579585\n",
      "3.6932847726275213\t0.23076923068047336\t0.02439024390144755\t0.044117647055579585\n",
      "3.7025762221892364\t0.22222222213991769\t0.02439024390144755\t0.04395604395282373\n",
      "3.7071248528664\t0.24999999991071428\t0.02845528455168881\t0.051094890507219354\n",
      "3.7182208106969483\t0.24999999991071428\t0.02845528455168881\t0.051094890507219354\n",
      "3.7256669460912235\t0.2758620688703924\t0.032520325201930066\t0.05818181817758678\n",
      "3.7350056230206974\t0.26666666657777777\t0.032520325201930066\t0.05797101448855283\n",
      "3.749613192223478\t0.29032258055150884\t0.03658536585217133\t0.06498194945379192\n",
      "3.759581410617102\t0.31249999990234373\t0.040650406502412584\t0.07194244603798974\n",
      "3.7705744545091875\t0.31249999990234373\t0.040650406502412584\t0.07194244603798974\n",
      "3.7894241812755354\t0.31249999990234373\t0.040650406502412584\t0.07194244603798974\n",
      "3.7983005273272283\t0.31249999990234373\t0.040650406502412584\t0.07194244603798974\n",
      "3.8059091822360642\t0.31249999990234373\t0.040650406502412584\t0.07194244603798974\n",
      "3.8103634163853712\t0.3333333332323232\t0.04471544715265384\t0.07885304658932951\n",
      "3.820259876374621\t0.3333333332407407\t0.0487804878028951\t0.08510638297268748\n",
      "3.8251821844023652\t0.31578947360110804\t0.0487804878028951\t0.08450704224756993\n",
      "3.841228077013511\t0.299999999925\t0.0487804878028951\t0.08391608391021566\n",
      "3.8473036524956115\t0.3170731706543724\t0.05284552845313636\t0.09059233448846046\n",
      "3.8553082480211742\t0.31818181810950413\t0.05691056910337762\t0.09655172413127229\n",
      "3.8662742146407254\t0.30434782602079397\t0.05691056910337762\t0.09589041095233629\n",
      "3.8734380088862963\t0.2857142856559767\t0.05691056910337762\t0.09491525423085322\n",
      "3.8853154683602042\t0.2857142856559767\t0.05691056910337762\t0.09491525423085322\n",
      "3.89638743159594\t0.279999999944\t0.05691056910337762\t0.09459459458820306\n",
      "3.9004538457957096\t0.279999999944\t0.05691056910337762\t0.09459459458820306\n",
      "3.9102678148192354\t0.279999999944\t0.05691056910337762\t0.09459459458820306\n",
      "3.9187169250217266\t0.279999999944\t0.05691056910337762\t0.09459459458820306\n",
      "3.925204146944452\t0.279999999944\t0.05691056910337762\t0.09459459458820306\n",
      "3.9344430307974108\t0.2941176470011534\t0.060975609753618876\t0.10101010100329898\n",
      "3.9443819972802885\t0.3018867923958704\t0.06504065040386013\t0.10702341136407868\n",
      "3.949760156974662\t0.30909090903471076\t0.06910569105410139\t0.11295681062372377\n",
      "3.9544638514635153\t0.31034482753269915\t0.07317073170434266\t0.11842105262378809\n",
      "3.966503099363763\t0.29508196716474067\t0.07317073170434266\t0.11726384364056915\n",
      "3.97998952382477\t0.29230769226272185\t0.07723577235458391\t0.12218649516899122\n",
      "3.9919570654747076\t0.2835820895099131\t0.07723577235458391\t0.1214057507909645\n",
      "4.001503203820903\t0.2999999999571428\t0.08536585365506642\t0.13291139239665115\n",
      "4.005987205368001\t0.2999999999571428\t0.08536585365506642\t0.13291139239665115\n",
      "4.011230105825234\t0.2999999999571428\t0.08536585365506642\t0.13291139239665115\n",
      "4.01888475130545\t0.2999999999571428\t0.08536585365506642\t0.13291139239665115\n",
      "4.0287224792991765\t0.2941176470155709\t0.08130081300482517\t0.1273885350237332\n",
      "4.03754984709667\t0.2941176470155709\t0.08130081300482517\t0.1273885350237332\n",
      "4.046301885799039\t0.30882352936634944\t0.08536585365506642\t0.13375796177491986\n",
      "4.062464633316267\t0.30882352936634944\t0.08536585365506642\t0.13375796177491986\n",
      "4.079763525456656\t0.30882352936634944\t0.08536585365506642\t0.13375796177491986\n",
      "4.094135287508834\t0.3043478260428481\t0.08536585365506642\t0.13333333332486771\n",
      "4.108939891972113\t0.3142857142408163\t0.08943089430530768\t0.13924050632030122\n",
      "4.121499273285735\t0.28947368417243763\t0.08943089430530768\t0.1366459627244319\n",
      "4.129582038440276\t0.2962962962597165\t0.0975609756057902\t0.14678899081671015\n",
      "4.1365512062911876\t0.284090909058626\t0.10162601625603146\t0.1497005987934311\n",
      "4.148866955481935\t0.2708333333051215\t0.10569105690627272\t0.15204678361683935\n",
      "4.155924449965823\t0.262135922304647\t0.10975609755651397\t0.15472779368740816\n",
      "4.164974816434551\t0.2660550458471509\t0.1178861788569965\t0.16338028168093632\n",
      "4.17786539561348\t0.260869565194707\t0.12195121950723775\t0.16620498614037646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1913021805812605\t0.2586206896328775\t0.12195121950723775\t0.16574585634443392\n",
      "4.201412447087932\t0.2749999999770833\t0.13414634145796153\t0.18032786884260504\n",
      "4.214068713888992\t0.26984126981985385\t0.13821138210820277\t0.18279569891490344\n",
      "4.221468032163102\t0.2692307692100592\t0.14227642275844404\t0.18617021275605478\n",
      "4.22399089630926\t0.2651515151314279\t0.14227642275844404\t0.185185185175387\n",
      "4.234709732758347\t0.2651515151314279\t0.14227642275844404\t0.185185185175387\n",
      "4.241469155123923\t0.2666666666469136\t0.1463414634086853\t0.1889763779428359\n",
      "4.250752956664655\t0.2666666666469136\t0.1463414634086853\t0.1889763779428359\n",
      "4.258226457575802\t0.2666666666469136\t0.1463414634086853\t0.1889763779428359\n",
      "4.260824527882505\t0.26865671639786143\t0.1463414634086853\t0.18947368420055402\n",
      "4.270528179884423\t0.27272727270661157\t0.1463414634086853\t0.1904761904661124\n",
      "4.276657019101549\t0.2713178294363319\t0.14227642275844404\t0.18666666665671108\n",
      "4.279882530041505\t0.2734374999786377\t0.14227642275844404\t0.18716577539106063\n",
      "4.2839818026986904\t0.26399999997887996\t0.13414634145796153\t0.17789757411439908\n",
      "4.290502367366571\t0.2601626016048648\t0.13008130080772026\t0.17344173440794353\n",
      "4.2982032255386\t0.24576271184357942\t0.1178861788569965\t0.15934065933190433\n",
      "4.304936979722697\t0.24786324784206296\t0.1178861788569965\t0.15977961431626558\n",
      "4.3129747759667225\t0.2477876105975409\t0.11382113820675524\t0.15598885793002848\n",
      "4.323872989800293\t0.2522522522295268\t0.11382113820675524\t0.15686274508925138\n",
      "4.329055837646592\t0.24074074071844992\t0.10569105690627272\t0.14689265535893262\n",
      "4.342097380489577\t0.2475247524507401\t0.10162601625603146\t0.1440922190118679\n",
      "4.347704598971177\t0.2553191489090086\t0.0975609756057902\t0.1411764705799308\n",
      "4.351557918183971\t0.2857142856802721\t0.0975609756057902\t0.14545454544573003\n",
      "4.359830446832348\t0.26315789470221607\t0.08130081300482517\t0.12422360247675629\n",
      "4.363914965477306\t0.2575757575367309\t0.06910569105410139\t0.10897435896737342\n",
      "4.369317909528036\t0.2622950819242139\t0.06504065040386013\t0.10423452768050588\n",
      "4.378514643583912\t0.24489795913369428\t0.0487804878028951\t0.08135593219787417\n",
      "4.386325675819535\t0.2272727272210744\t0.040650406502412584\t0.06896551723662306\n",
      "4.390596385404933\t0.23076923071005917\t0.03658536585217133\t0.06315789473240999\n",
      "4.401919867203105\t0.20588235288062284\t0.02845528455168881\t0.049999999996428575\n",
      "4.4079257737030275\t0.17241379304399523\t0.020325203251206292\t0.036363636360991734\n",
      "4.4151611205306835\t0.20833333324652778\t0.020325203251206292\t0.037037037034293556\n",
      "4.426856496196706\t0.22727272716942148\t0.020325203251206292\t0.03731343283303631\n",
      "4.438525453966577\t0.1999999999\t0.016260162600965033\t0.03007518796766352\n",
      "4.451884042297024\t0.16666666657407406\t0.012195121950723775\t0.022727272725550963\n",
      "4.4596992412698455\t0.16666666657407406\t0.012195121950723775\t0.022727272725550963\n",
      "4.468411261041183\t0.1764705881314879\t0.012195121950723775\t0.022813688211192874\n",
      "4.476659552019555\t0.1764705881314879\t0.012195121950723775\t0.022813688211192874\n",
      "4.489856964733917\t0.1764705881314879\t0.012195121950723775\t0.022813688211192874\n",
      "4.508592017751653\t0.1764705881314879\t0.012195121950723775\t0.022813688211192874\n",
      "4.526706878852565\t0.1764705881314879\t0.012195121950723775\t0.022813688211192874\n",
      "4.543815406796057\t0.19999999986666667\t0.012195121950723775\t0.022988505745364866\n",
      "4.550252124958206\t0.21428571413265304\t0.012195121950723775\t0.02307692307514793\n",
      "4.553811474062968\t0.2727272724793388\t0.012195121950723775\t0.023346303500128694\n",
      "4.559106121596415\t0.2727272724793388\t0.012195121950723775\t0.023346303500128694\n",
      "4.573803619307\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "4.587529384356458\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "4.5994529741001315\t0.2222222219753086\t0.008130081300482516\t0.015686274508573624\n",
      "4.613179382693488\t0.2222222219753086\t0.008130081300482516\t0.015686274508573624\n",
      "4.625011303287465\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "4.639914754603524\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "4.650637345213909\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "4.659067194734234\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "4.663604353845585\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "4.666456736915279\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "4.670106311386917\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "4.675734260643367\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "4.682059624756221\t0.12499999984374999\t0.004065040650241258\t0.007874015747411494\n",
      "4.697495961154345\t0.12499999984374999\t0.004065040650241258\t0.007874015747411494\n",
      "4.70451925607631\t0.2222222219753086\t0.008130081300482516\t0.015686274508573624\n",
      "4.7136608334840275\t0.2222222219753086\t0.008130081300482516\t0.015686274508573624\n",
      "4.722861665359233\t0.2222222219753086\t0.008130081300482516\t0.015686274508573624\n",
      "4.729238202853594\t0.2222222219753086\t0.008130081300482516\t0.015686274508573624\n",
      "4.736330677347723\t0.24999999968749997\t0.008130081300482516\t0.015748031494822987\n",
      "4.744332739675883\t0.24999999968749997\t0.008130081300482516\t0.015748031494822987\n",
      "4.757897865551058\t0.24999999968749997\t0.008130081300482516\t0.015748031494822987\n",
      "4.762550451385323\t0.24999999968749997\t0.008130081300482516\t0.015748031494822987\n",
      "4.765706212550867\t0.24999999968749997\t0.008130081300482516\t0.015748031494822987\n",
      "4.779407586262096\t0.24999999968749997\t0.008130081300482516\t0.015748031494822987\n",
      "4.789682125032414\t0.24999999968749997\t0.008130081300482516\t0.015748031494822987\n",
      "4.794546407938469\t0.16666666638888888\t0.004065040650241258\t0.007936507935878055\n",
      "4.806527046777774\t0.16666666638888888\t0.004065040650241258\t0.007936507935878055\n",
      "4.812226540700067\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "4.822408893436659\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "4.829272182949353\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "4.834721296385396\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "4.840836072165985\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "4.85590800998034\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.8590120515436865\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.874795391282532\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.884210458083544\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.888109999301378\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.893213303817902\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.898967585584614\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.905017330369446\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.908972143253777\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.919047612755094\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.929453644959722\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.940882741531823\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.947153918503318\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.9555569682852365\t0.3333333322222222\t0.004065040650241258\t0.008032128513411074\n",
      "4.9655548455775715\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "4.981841919070575\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "4.9874677761108615\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "4.993259534763638\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.005173994752113\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.008797924325336\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.019020095758606\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.034193200350273\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.038981003977824\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.048223279707599\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.064534910314251\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.071469463699032\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.084090632677544\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.0918894915957935\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.09483755758265\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.107487565896008\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.1129936706856824\t0.249999999375\t0.004065040650241258\t0.007999999999359998\n",
      "5.123479027359281\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.136075992195401\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.144047060108278\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.158512350230012\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.165942797961179\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.172935698239598\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.181680994166527\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.192159578262363\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.203959805949125\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.213191552145872\t0.1999999996\t0.004065040650241258\t0.00796812748940493\n",
      "5.220209335966501\t0.16666666638888888\t0.004065040650241258\t0.007936507935878055\n",
      "5.226010372454766\t0.16666666638888888\t0.004065040650241258\t0.007936507935878055\n",
      "5.232524034159724\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "5.234930370061193\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "5.244708983285818\t0.12499999984374999\t0.004065040650241258\t0.007874015747411494\n",
      "5.260721115453634\t0.2222222219753086\t0.008130081300482516\t0.015686274508573624\n",
      "5.266377566440497\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.273468715662602\t0.24999999979166665\t0.012195121950723775\t0.0232558139516856\n",
      "5.280678048788104\t0.24999999979166665\t0.012195121950723775\t0.0232558139516856\n",
      "5.284030894574244\t0.24999999979166665\t0.012195121950723775\t0.0232558139516856\n",
      "5.2926286941510625\t0.24999999979166665\t0.012195121950723775\t0.0232558139516856\n",
      "5.3004757247981615\t0.30769230745562126\t0.016260162600965033\t0.03088803088564571\n",
      "5.304958933324087\t0.30769230745562126\t0.016260162600965033\t0.03088803088564571\n",
      "5.30796076921979\t0.30769230745562126\t0.016260162600965033\t0.03088803088564571\n",
      "5.3162293453351595\t0.30769230745562126\t0.016260162600965033\t0.03088803088564571\n",
      "5.321662083326373\t0.30769230745562126\t0.016260162600965033\t0.03088803088564571\n",
      "5.326021945162211\t0.3333333330555555\t0.016260162600965033\t0.03100775193558079\n",
      "5.341410725435708\t0.1999999998\t0.008130081300482516\t0.015624999998779298\n",
      "5.344706770323683\t0.2222222219753086\t0.008130081300482516\t0.015686274508573624\n",
      "5.346335129754152\t0.24999999968749997\t0.008130081300482516\t0.015748031494822987\n",
      "5.352820391242858\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "5.359902263095137\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "5.369130001810845\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "5.372981643944513\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "5.3770547357271425\t0.14285714265306124\t0.004065040650241258\t0.007905138339296037\n",
      "5.383640130225103\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.3912439022096805\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.398014392412733\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.405161143455189\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.40639868035214\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.4095528545440175\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.411526404612232\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.413954902731348\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.417804868484382\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.422820036357734\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.429059814254288\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n",
      "5.432432869274635\t0.29999999969999996\t0.012195121950723775\t0.023437499998168947\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-22b967ba8110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-559373087515>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(training_data, test_data, iterations)\u001b[0m\n\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtextcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 evaluation_results = evaluate_model(\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mtextcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtextcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-cf1ced310fc4>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(tokenizer, textcat, test_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrue_negatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfalse_negatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cats\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpredicted_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.TextCategorizer.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(seqs_in)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train, test = load_training_data(limit=2500)\n",
    "    train_model(train, test)\n",
    "    print(\"Testing model\")\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
